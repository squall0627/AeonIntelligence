import asyncio
import logging
import os
import shutil

from pathlib import Path
from pprint import PrettyPrinter
from typing import Any, AsyncGenerator, Callable, Dict, Self, Type, Union
from rich.console import Console
from rich.panel import Panel
from rich.tree import Tree
from uuid import UUID, uuid4
from dataclasses import dataclass

from langchain_core.documents import Document
from langchain_core.messages import AIMessage, HumanMessage

from core.ai_core.embedder.embedder_base import EmbedderBase
from core.ai_core.files import AIFile
from core.ai_core.files.file import load_aifile
from core.ai_core.knowledge_warehouse.serialization import KWSerialized
from core.ai_core.llm.llm_endpoint import LLMEndpoint, LLMInfo, default_rag_llm
from core.ai_core.processor.processor_registry import get_processor_class
from core.ai_core.rag.config.ai_rag_config import RetrievalConfig
from core.ai_core.rag.ai_rag_langgraph import AiQARAGLangGraph
from core.ai_core.rag.entities.chat import ChatHistoryInfo, ChatHistory
from core.ai_core.rag.entities.models import (
    SearchResult,
    AIKnowledge,
    ParsedRAGChunkResponse,
    ParsedRAGResponse,
)
from core.ai_core.storage.storage_base import StorageBase, StorageInfo
from core.ai_core.storage.storage_builder import StorageBuilder
from core.ai_core.embedder.embedder_builder import EmbedderBuilder
from core.ai_core.vectordb.vectordb_base import VectordbBase
from core.ai_core.vectordb.vectordb_builder import VectordbBuilder

logger = logging.getLogger("ai_core")


async def process_file(
    file: AIFile, **processor_kwargs: dict[str, Any]
) -> list[Document]:
    """
    ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã€‚
    ã“ã®é–¢æ•°ã¯ AIFile ã‚’å—ã‘å–ã‚Šã€Langchain ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚
    å¼•æ•°:
    - file (AIFile): å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚
    - processor_kwargs (dict[str, Any]): ãƒ—ãƒ­ã‚»ãƒƒã‚µã¸ã®è¿½åŠ ã®å¼•æ•°ã€‚

    æˆ»ã‚Šå€¤:
    - list[Document]: Langchain ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå½¢å¼ã§å‡¦ç†ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã€‚

    ä¾‹å¤–:
    - ValueError: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã§ããšã€ã‹ã¤ skip_file_error ãŒ False ã®å ´åˆã€‚
    - Exception: ç‰¹å®šã®ç¨®é¡žã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µãŒè¦‹ã¤ã‹ã‚‰ãšã€ã‹ã¤ skip_file_error ãŒ False ã®å ´åˆã€‚
    """

    knowledge = []

    try:
        if file.file_extension:
            processor_cls = get_processor_class(file.file_extension)
            logger.debug(f"processing {file} using class {processor_cls.__name__}")
            processor = processor_cls(**processor_kwargs)
            docs = await processor.process_file(file)
            knowledge.extend(docs)
        else:
            logger.error(f"can't find processor for {file}")
            raise ValueError(f"can't parse {file}. can't find file extension")
    except KeyError as e:
        raise Exception(f"Can't parse {file}. No available processor") from e

    return knowledge


@dataclass
class KnowledgeWarehouseInfo:
    kw_id: UUID
    kw_name: str
    chats_info: ChatHistoryInfo
    llm_info: LLMInfo
    files_info: StorageInfo | None = None

    def to_tree(self):
        tree = Tree("ðŸ“Š Knowledge Warehouse Information")
        tree.add(f"ðŸ†” ID: [bold cyan]{self.kw_id}[/bold cyan]")
        tree.add(
            f"ðŸ§  Knowledge Warehouse Name: [bold green]{self.kw_name}[/bold green]"
        )

        if self.files_info:
            files_tree = tree.add("ðŸ“ Files")
            self.files_info.add_to_tree(files_tree)

        chats_tree = tree.add("ðŸ’¬ Chats")
        self.chats_info.add_to_tree(chats_tree)

        llm_tree = tree.add("ðŸ¤– LLM")
        self.llm_info.add_to_tree(llm_tree)
        return tree


class KnowledgeWarehouse:
    """
    KnowledgeWarehouse ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹ã€‚
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯æƒ…å ±ã‚’å–å¾—ã—ãŸã„çŸ¥è­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã® KnowledgeWarehouse ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚æ©Ÿèƒ½ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ï¼š
    - ä»»æ„ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã€S3 ãªã©ï¼‰ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã€‚
    - ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€ã•ã¾ã–ã¾ãªå½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã€‚
    - å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä»»æ„ã® vector storeï¼ˆFAISSã€Pinecone ãªã©ï¼‰ã«ä¿å­˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ FAISSï¼‰ã€‚
    - å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã€‚
    - *Aeon Intelligence* ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€RAG(retrieval augmented generation) æ¤œç´¢ã‚’è¡Œã†ã€‚

    KnowledgeWarehouse ã¯ä»¥ä¸‹ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ï¼š
    - vector store å†…ã§ã®æƒ…å ±æ¤œç´¢ã€‚
    - KnowledgeWarehouse å†…ã®çŸ¥è­˜ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ã€‚
    - è³ªå•ã¸ã®å›žç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§æä¾›ã€‚

    ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£:
    - name (str): KnowledgeWarehouse ã®åå‰ã€‚
    - kw_id (UUID): KnowledgeWarehouse ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚
    - storage (StorageBase): ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚
    - llm (LLMEndpoint): å›žç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹è¨€èªžãƒ¢ãƒ‡ãƒ«ã€‚
    - vector_db (VectordbBase): å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ vector storeã€‚
    - embedder (EmbedderBase): å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ Embeddingsã€‚
    """

    def __init__(
        self,
        *,
        name: str,
        llm: LLMEndpoint,
        kw_id: UUID | None = None,
        vector_db: VectordbBase | None = None,
        embedder: EmbedderBase | None = None,
        storage: StorageBase | None = None,
        kw_path: str | Path | None = None,
    ):
        self.kw_id = kw_id
        self.name = name
        self.storage = storage

        # Chat å±¥æ­´
        self._chats = self._init_chats()
        self.default_chat = list(self._chats.values())[0]

        # RAG dependencies:
        self.llm = llm
        self.vector_db = vector_db
        self.embedder = embedder

        # Path to the folder where the KW is saved
        self.kw_path = kw_path

    def __repr__(self) -> str:
        pp = PrettyPrinter(width=80, depth=None, compact=False, sort_dicts=False)
        return pp.pformat(self.info())

    def print_info(self):
        console = Console()
        tree = self.info().to_tree()
        panel = Panel(
            tree, title="Knowledge Warehouse Info", expand=False, border_style="bold"
        )
        console.print(panel)

    @classmethod
    def load(cls, folder_path: str | Path) -> Self:
        """
        ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‹ã‚‰ KnowledgeWarehouse ã‚’èª­ã¿è¾¼ã‚€ã€‚
        å¼•æ•°:
        - folder_path (str | Path): KnowledgeWarehouse ã‚’å«ã‚€ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ãƒ‘ã‚¹ã€‚

        æˆ»ã‚Šå€¤:
        - KnowledgeWarehouse: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¾ã‚ŒãŸ KnowledgeWarehouseã€‚

        ä¾‹:
        ```python
        kw_loaded = KnowledgeWarehouse.load("path/to/KnowledgeWarehouse")
        kw_loaded.print_info()
        ```
        """
        if isinstance(folder_path, str):
            folder_path = Path(folder_path)
        if not folder_path.exists():
            raise ValueError(f"path {folder_path} doesn't exist")

        # Load KWSerialized
        with open(os.path.join(folder_path, "config.json"), "r") as f:
            kw_serialized = KWSerialized.model_validate_json(f.read())

        # Loading storage
        storage = StorageBuilder.load_storage(kw_serialized.storage_config)

        # Load Embedder
        embedder = EmbedderBuilder.load_embedder(kw_serialized.embedding_config)

        # Load vector db
        vector_db = VectordbBuilder.load_vectordb(
            kw_serialized.vectordb_config, embedder.embedder
        )

        return cls(
            kw_id=kw_serialized.kw_id,
            name=kw_serialized.kw_name,
            embedder=embedder,
            llm=LLMEndpoint.from_config(kw_serialized.llm_config),
            storage=storage,
            vector_db=vector_db,
            kw_path=folder_path,
        )

    async def save(self, folder_path: str | Path):
        """
        Knowledge Warehouse ã‚’ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚

        å¼•æ•°:
        - folder_path (str | Path): Knowledge Warehouse ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚

        æˆ»ã‚Šå€¤:
        - str: Knowledge Warehouse ãŒä¿å­˜ã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚

        ä¾‹:
        ```python
        await kw.save("path/to/KnowledgeWarehouse")
        ```
        """
        if isinstance(folder_path, str):
            folder_path = Path(folder_path)

        kw_path = os.path.join(folder_path, f"kw_{self.kw_id}")
        os.makedirs(kw_path, exist_ok=True)

        self.kw_path = kw_path

        # Save serialized vector db
        vectordb_config = await self.vector_db.save(kw_path)

        # Save serialized embedder
        embedder_config = self.embedder.save()

        # Save serialized storage
        storage_config = StorageBuilder.save_storage(self.storage)

        kw_serialized = KWSerialized(
            kw_id=self.kw_id,
            kw_name=self.name,
            chat_history=self.chat_history.get_chat_history(),
            llm_config=self.llm.get_config(),
            vectordb_config=vectordb_config,
            embedding_config=embedder_config,
            storage_config=storage_config,
        )

        with open(os.path.join(kw_path, "config.json"), "w") as f:
            f.write(kw_serialized.model_dump_json())
        return kw_path

    async def delete(self) -> None:
        """Delete the entire knowledge warehouse including all files and vectors."""
        try:
            # Delete the storage directory if it exists
            if self.storage and self.storage.get_directory_path():
                storage_root = self.storage.get_directory_path()
                kw_storage_file_path = os.path.join(storage_root, str(self.kw_id))
                if os.path.exists(kw_storage_file_path):
                    shutil.rmtree(kw_storage_file_path)

            # Delete vector store if it exists
            self.vector_db.vector_db.delete(self.vector_db.get_all_ids())

            # Delete all files under self.kw_path
            if os.path.exists(self.kw_path):
                shutil.rmtree(self.kw_path)

        except Exception as e:
            logger.error(f"Error deleting knowledge warehouse: {self.name} {str(e)}")
            raise e

    def info(self) -> KnowledgeWarehouseInfo:
        # TODO: embedding
        chats_info = ChatHistoryInfo(
            nb_chats=len(self._chats),
            current_default_chat=self.default_chat.id,
            current_chat_history_length=len(self.default_chat),
        )

        return KnowledgeWarehouseInfo(
            kw_id=self.kw_id,
            kw_name=self.name,
            files_info=self.storage.info() if self.storage else None,
            chats_info=chats_info,
            llm_info=self.llm.info(),
        )

    @property
    def chat_history(self) -> ChatHistory:
        return self.default_chat

    def get_chat_history(self, chat_id: UUID):
        return self._chats[chat_id]

    def _init_chats(self) -> Dict[UUID, ChatHistory]:
        chat_id = uuid4()
        default_chat = ChatHistory(chat_id=chat_id, kw_id=self.kw_id)
        return {chat_id: default_chat}

    @classmethod
    async def afrom_files(
        cls,
        *,
        name: str,
        file_paths: list[str | Path],
        vector_db: VectordbBase | None = None,
        storage: StorageBase = StorageBuilder.build_default_storage(None, None),
        llm: LLMEndpoint | None = None,
        embedder: EmbedderBase | None = None,
        skip_file_error: bool = False,
        processor_kwargs: dict[str, Any] | None = None,
    ):
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‹ã‚‰ KnowledgeWarehouse ã‚’ä½œæˆã™ã‚‹ã€‚
        å¼•æ•°:
        - name (str): KnowledgeWarehouse ã®åå‰ã€‚
        - file_paths (list[str | Path]): KnowledgeWarehouse ã«è¿½åŠ ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã€‚
        - vector_db (VectordbBase | None): å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ VectorStoreã€‚
        - storage (StorageBase): ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚
        - llm (LLMEndpoint | None): å›žç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹è¨€èªžãƒ¢ãƒ‡ãƒ«ã€‚
        - embedder (Embeddings | None): å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ Embeddingsã€‚
        - skip_file_error (bool): å‡¦ç†ã§ããªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹ã€‚
        - processor_kwargs (dict[str, Any] | None): ãƒ—ãƒ­ã‚»ãƒƒã‚µã¸ã®è¿½åŠ ã®å¼•æ•°ã€‚

        æˆ»ã‚Šå€¤:
        - KnowledgeWarehouse: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ä½œæˆã•ã‚ŒãŸ KnowledgeWarehouseã€‚

        ä¾‹:
        ```python
        kw = await KnowledgeWarehouse.afrom_files(name="My Knowledge Warehouse", file_paths=["file1.pdf", "file2.pdf"])
        kw.print_info()
        ```
        """
        if llm is None:
            llm = default_rag_llm()

        if embedder is None:
            embedder = EmbedderBuilder.build_default_embedder()

        kw_id = uuid4()

        # Add files to storage and vector db
        vector_db = await cls._add_file_to_storage_and_vectordb(
            kw_id=kw_id,
            file_paths=file_paths,
            storage=storage,
            embedder=embedder,
            skip_file_error=skip_file_error,
            processor_kwargs=processor_kwargs,
        )

        return cls(
            kw_id=kw_id,
            name=name,
            storage=storage,
            llm=llm,
            embedder=embedder,
            vector_db=vector_db,
        )

    @classmethod
    def from_files(
        cls,
        *,
        name: str,
        file_paths: list[str | Path],
        vector_db: VectordbBase | None = None,
        storage: StorageBase = StorageBuilder.build_default_storage(None, None),
        llm: LLMEndpoint | None = None,
        embedder: EmbedderBase | None = None,
        skip_file_error: bool = False,
        processor_kwargs: dict[str, Any] | None = None,
    ) -> Self:
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            cls.afrom_files(
                name=name,
                file_paths=file_paths,
                vector_db=vector_db,
                storage=storage,
                llm=llm,
                embedder=embedder,
                skip_file_error=skip_file_error,
                processor_kwargs=processor_kwargs,
            )
        )

    async def asearch(
        self,
        query: str | Document,
        n_results: int = 5,
        search_filter: Callable | Dict[str, Any] | None = None,
        fetch_n_neighbors: int = 20,
    ) -> list[SearchResult]:
        """
        ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦ KnowledgeWarehouse å†…ã®é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã™ã‚‹ã€‚
        å¼•æ•°:
        - query (str | Document): æ¤œç´¢ã™ã‚‹ã‚¯ã‚¨ãƒªã€‚
        - n_results (int): è¿”ã™çµæžœã®æ•°ã€‚
        - search_filter (Callable | Dict[str, Any] | None): æ¤œç´¢ã«é©ç”¨ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ã€‚
        - fetch_n_neighbors (int): å–å¾—ã™ã‚‹è¿‘å‚ã®æ•°ã€‚

        æˆ»ã‚Šå€¤:
        - list[SearchResult]: å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã€‚

        ä¾‹:
        ```python
        kw = KnowledgeWarehouse.from_files(name="My Knowledge Warehouse", file_paths=["file1.pdf", "file2.pdf"])
        results = await kw.asearch("What is your name?")
        for result in results:
            print(result.chunk.page_content)
        ```
        """
        if not self.vector_db:
            raise ValueError("No vector db configured for this Knowledge Warehouse")

        result = await self.vector_db.vector_db.asimilarity_search_with_score(
            query, k=n_results, filter=search_filter, fetch_k=fetch_n_neighbors
        )

        return [SearchResult(chunk=d, distance=s) for d, s in result]

    async def ask_streaming(
        self,
        question: str,
        retrieval_config: RetrievalConfig | None = None,
        rag_pipeline: Type[Union[AiQARAGLangGraph]] | None = None,
        list_files: list[AIKnowledge] | None = None,
        chat_history: ChatHistory | None = None,
    ) -> AsyncGenerator[ParsedRAGChunkResponse, ParsedRAGChunkResponse]:
        """
        KnowledgeWarehouse ã«è³ªå•ã‚’ã—ã¦ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§ç”Ÿæˆã•ã‚ŒãŸå›žç­”ã‚’å–å¾—ã™ã‚‹ã€‚
        å¼•æ•°:
        - question (str): è³ªå•å†…å®¹ã€‚
        - retrieval_config (RetrievalConfig | None): æ¤œç´¢è¨­å®šï¼ˆè©³ç´°ã¯ RetrievalConfig ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ï¼‰ã€‚
        - rag_pipeline (Type[Union[AiQARAGLangGraph]] | None): ä½¿ç”¨ã™ã‚‹ RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
        - list_files (list[AiKnowledge] | None): RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã€‚
        - chat_history (ChatHistory | None): ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€‚

        æˆ»ã‚Šå€¤:
        - AsyncGenerator[ParsedRAGChunkResponse, ParsedRAGChunkResponse]: ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§ç”Ÿæˆã•ã‚ŒãŸå›žç­”ã€‚

        ä¾‹:
        ```python
        kw = KnowledgeWarehouse.from_files(name="My Knowledge Warehouse", file_paths=["file1.pdf", "file2.pdf"])
        async for chunk in kw.ask_streaming("What is your name?"):
            print(chunk.answer)
        ```
        """
        llm = self.llm

        # åˆ¥ã® LLM ãƒ¢ãƒ‡ãƒ«ã‚’æ¸¡ã—ãŸå ´åˆã€KnowledgeWarehouse ã®ãƒ¢ãƒ‡ãƒ«ãŒä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚
        if retrieval_config:
            if retrieval_config.llm_config != self.llm.get_config():
                llm = LLMEndpoint.from_config(config=retrieval_config.llm_config)
        else:
            retrieval_config = RetrievalConfig(llm_config=self.llm.get_config())

        if rag_pipeline is None:
            rag_pipeline = AiQARAGLangGraph

        rag_instance = rag_pipeline(
            retrieval_config=retrieval_config,
            llm=llm,
            vector_store=self.vector_db.vector_db,
        )

        chat_history = self.default_chat if chat_history is None else chat_history
        list_files = [] if list_files is None else list_files

        full_answer = ""

        async for response in rag_instance.answer_astream(
            question=question, history=chat_history, list_files=list_files
        ):
            # Format output
            if not response.last_chunk:
                yield response
            full_answer += response.answer

        chat_history.append(HumanMessage(content=question))
        chat_history.append(AIMessage(content=full_answer))
        yield response

    async def aask_streaming(
        self,
        question: str,
        retrieval_config: RetrievalConfig | None = None,
        rag_pipeline: Type[Union[AiQARAGLangGraph]] | None = None,
        list_files: list[AIKnowledge] | None = None,
        chat_history: ChatHistory | None = None,
    ) -> ParsedRAGResponse:
        """
        ask_streamingã®åŒæœŸåŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³.
        å¼•æ•°:
        - question (str): è³ªå•å†…å®¹ã€‚
        - retrieval_config (RetrievalConfig | None): æ¤œç´¢è¨­å®šï¼ˆè©³ç´°ã¯ RetrievalConfig ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ï¼‰ã€‚
        - rag_pipeline (Type[Union[AiQARAGLangGraph]] | None): ä½¿ç”¨ã™ã‚‹ RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
        - list_files (list[AiKnowledge] | None): RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã€‚
        - chat_history (ChatHistory | None): ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€‚

        æˆ»ã‚Šå€¤:
        - ParsedRAGResponse: ç”Ÿæˆã•ã‚ŒãŸå›žç­”ã€‚
        """
        full_answer = ""
        metadata = None

        async for response in self.ask_streaming(
            question=question,
            retrieval_config=retrieval_config,
            rag_pipeline=rag_pipeline,
            list_files=list_files,
            chat_history=chat_history,
        ):
            full_answer += response.answer
            if response.last_chunk:
                metadata = response.metadata

        return ParsedRAGResponse(answer=full_answer, metadata=metadata)

    def ask(
        self,
        question: str,
        retrieval_config: RetrievalConfig | None = None,
        rag_pipeline: Type[Union[AiQARAGLangGraph]] | None = None,
        list_files: list[AIKnowledge] | None = None,
        chat_history: ChatHistory | None = None,
    ) -> ParsedRAGResponse:
        """
        ask_streamingã®å®Œå…¨åŒæœŸåŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³.
        å¼•æ•°:
        - question (str): è³ªå•å†…å®¹ã€‚
        - retrieval_config (RetrievalConfig | None): æ¤œç´¢è¨­å®šï¼ˆè©³ç´°ã¯ RetrievalConfig ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ï¼‰ã€‚
        - rag_pipeline (Type[Union[AiQARAGLangGraph]] | None): ä½¿ç”¨ã™ã‚‹ RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
        - list_files (list[AiKnowledge] | None): RAG ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã€‚
        - chat_history (ChatHistory | None): ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€‚

        æˆ»ã‚Šå€¤:
        - ParsedRAGResponse: ç”Ÿæˆã•ã‚ŒãŸå›žç­”ã€‚
        """
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.aask_streaming(
                question=question,
                retrieval_config=retrieval_config,
                rag_pipeline=rag_pipeline,
                list_files=list_files,
                chat_history=chat_history,
            )
        )

    @classmethod
    async def _add_file_to_storage_and_vectordb(
        cls,
        kw_id: UUID,
        file_paths: list[str | Path],
        storage: StorageBase,
        vector_db: VectordbBase | None = None,
        embedder: EmbedderBase | None = None,
        skip_file_error: bool = False,
        processor_kwargs: dict[str, Any] | None = None,
    ) -> VectordbBase | None:

        processor_kwargs = processor_kwargs or {}

        for path in file_paths:
            file = await load_aifile(kw_id, path)
            await storage.upload_file(file)

            logger.debug(f"uploaded {file} to {storage}")

            try:
                # Parse files
                docs = await process_file(
                    file=file,
                    **processor_kwargs,
                )
            except Exception as e:
                if skip_file_error:
                    logger.warning(f"error processing {file}: {e}")
                    continue
                else:
                    raise e

            # Building KnowledgeWarehouse's vectordb
            if vector_db is None:
                vector_db = await VectordbBuilder.build_default_vectordb(
                    docs, embedder.embedder
                )
                ids = vector_db.get_all_ids()
            else:
                ids = await vector_db.vector_db.aadd_documents(docs)

            file.vectordb_ids = ids

            logger.debug(f"added {len(docs)} chunks to vectordb")

        return vector_db

    async def aadd_files(
        self,
        file_paths: list[str | Path],
        skip_file_error: bool = False,
        processor_kwargs: dict[str, Any] | None = None,
    ) -> None:
        await self._add_file_to_storage_and_vectordb(
            kw_id=self.kw_id,
            file_paths=file_paths,
            storage=self.storage,
            vector_db=self.vector_db,
            embedder=self.embedder,
            skip_file_error=skip_file_error,
            processor_kwargs=processor_kwargs,
        )

    async def delete_file(self, file: AIFile) -> None:
        # Remove file from storage
        await self.storage.remove_file(file.file_id)
        logger.debug(
            f"removed file {file.original_filename} from {self.name}'s storage"
        )

        # Remove file from vector db
        self.vector_db.vector_db.delete(file.vectordb_ids)
        logger.debug(
            f"removed file {file.original_filename} from {self.name}'s vector db"
        )
